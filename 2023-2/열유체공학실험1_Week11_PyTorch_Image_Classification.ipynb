{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JK-the-Ko/Thermo-Fluid-Dynamics-Experiment/blob/main/2023-2/%EC%97%B4%EC%9C%A0%EC%B2%B4%EA%B3%B5%ED%95%99%EC%8B%A4%ED%97%981_Week11_PyTorch_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ97dC_jLHvF"
      },
      "source": [
        "# Image Classification Using PyTorch Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX6s-j7fq3hA"
      },
      "source": [
        "## Check NVIDIA GPU Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_I7cz1Sq6S1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "D2rYJLwSYhb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pERmU-3xYjmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip Dataset"
      ],
      "metadata": {
        "id": "rOpdIRxMYnQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/dataset.zip\" # path to dataset"
      ],
      "metadata": {
        "id": "06rK1ga-YrPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Number of Data"
      ],
      "metadata": {
        "id": "_HjBulAhqmrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import join"
      ],
      "metadata": {
        "id": "19RtVbwlqqwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourcePath = \"/content/\" # path to dataset"
      ],
      "metadata": {
        "id": "TZyDYt1crDIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"<Training Dataset>\")\n",
        "for className in listdir(join(sourcePath, \"train\")) :\n",
        "  print(f\"{className}:{len(listdir(join(sourcePath, 'train', className)))}\")"
      ],
      "metadata": {
        "id": "nuLeStY3q_YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"<Test Dataset>\")\n",
        "for className in listdir(join(sourcePath, \"test\")) :\n",
        "  print(f\"{className}:{len(listdir(join(sourcePath, 'test', className)))}\")"
      ],
      "metadata": {
        "id": "_Pim74yHrsJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXUpV3puOOlX"
      },
      "source": [
        "## Create PyTorch DataLoader Class without Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhWPVY7tOKZc"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTjqdBsVOVUu"
      },
      "outputs": [],
      "source": [
        "class myDataLoader(Dataset) :\n",
        "  def __init__(self, opt, forMetric) :\n",
        "    super(myDataLoader, self).__init__()\n",
        "\n",
        "    self.opt = opt\n",
        "    self.forMetric = forMetric\n",
        "    self.imageDataset = self.getPathList()\n",
        "    self.label = {\"cloudy\":[1,0,0,0],\n",
        "                  \"desert\":[0,1,0,0],\n",
        "                  \"green_area\":[0,0,1,0],\n",
        "                  \"water\":[0,0,0,1]} # One-Hot Encoding\n",
        "\n",
        "  def __getitem__(self, index) :\n",
        "    image = Image.open(self.imageDataset[0][index]).convert(\"RGB\") #4D to 3D\n",
        "    image = self.transforms(image)\n",
        "\n",
        "    label = self.label[self.imageDataset[1][index]]\n",
        "\n",
        "    return {\"image\":image, \"label\":torch.as_tensor(label).float()}\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.imageDataset[1])\n",
        "\n",
        "  def getPathList(self) :\n",
        "    if self.forMetric :\n",
        "      classPath = join(self.opt[\"dataRoot\"], \"test\")\n",
        "    else :\n",
        "      classPath = join(self.opt[\"dataRoot\"], \"train\")\n",
        "\n",
        "    imagePathList, imageLabelList = [], []\n",
        "    for className in listdir(classPath) :\n",
        "      for imageName in listdir(join(classPath, className)) :\n",
        "        imagePathList.append(join(classPath, className, imageName))\n",
        "        imageLabelList.append(className)\n",
        "\n",
        "    return (imagePathList, imageLabelList)\n",
        "\n",
        "  def transforms(self, image) :\n",
        "    if self.forMetric :\n",
        "      myTransforms = transforms.Compose([transforms.Resize(self.opt[\"cropSize\"]),\n",
        "                                        transforms.ToTensor()]) # Resize Process for Test\n",
        "    else :\n",
        "      myTransforms = transforms.Compose([transforms.RandomCrop(self.opt[\"cropSize\"]),\n",
        "                                        transforms.ToTensor()]) # Random Crop for Training\n",
        "    image = myTransforms(image)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0-THJOPxvx"
      },
      "source": [
        "## Create PyTorch Image Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcjb8ky2PvGo"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMGcu1KP07P"
      },
      "outputs": [],
      "source": [
        "class myModel(nn.Module) :\n",
        "  def __init__(self, opt) :\n",
        "    super(myModel, self).__init__()\n",
        "\n",
        "    inputDim, targetDim, channels = opt[\"inputDim\"], opt[\"targetDim\"], opt[\"channels\"]\n",
        "\n",
        "    self.layer0 = nn.Sequential(nn.Conv2d(inputDim, channels, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(channels, channels*2, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(channels*2, channels*4, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer4 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer5 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer6 = nn.Linear(channels*4, targetDim)\n",
        "\n",
        "  def forward(self, input) :\n",
        "    output = self.layer0(input)\n",
        "    output = self.layer1(output)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    output = self.layer5(output)\n",
        "    output = F.adaptive_avg_pool2d(output, (1,1)).view(output.size(0), -1)\n",
        "    output = self.layer6(output)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wk4_cBzQU19"
      },
      "source": [
        "## Train DL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QJcgBk-QSsg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phpTOt47RjcB"
      },
      "source": [
        "### Fix Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81NJ5JxbRnaU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8F_Z5d3RlH-"
      },
      "outputs": [],
      "source": [
        "def fixSeed(seed) :\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Average Meter Instance"
      ],
      "metadata": {
        "id": "Jh4VCYXSswFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val*n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "oGUnMpzusxqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Accuracy Computation Function"
      ],
      "metadata": {
        "id": "fBn8NVKk8Pqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeAcc(pred, target) :\n",
        "  acc = (torch.argmax(pred, dim=1)==torch.argmax(target, dim=1)).sum()/pred.size(0)\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "B8qpzXhw8Org"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code as a Function (Abstraction)"
      ],
      "metadata": {
        "id": "lhJ28-fU6Zk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(opt, myDataLoader, myModel, criterion) :\n",
        "  fixSeed(opt[\"seed\"])\n",
        "\n",
        "  trainDataLoader = DataLoader(myDataLoader(opt, forMetric=False), batch_size=opt[\"batchSize\"], shuffle=True, drop_last=True)\n",
        "  testDataLoader = DataLoader(myDataLoader(opt, forMetric=True), batch_size=opt[\"batchSize\"], shuffle=False, drop_last=False)\n",
        "\n",
        "  fixSeed(opt[\"seed\"])\n",
        "  model = myModel(opt)\n",
        "  if opt[\"isCUDA\"] :\n",
        "    model = model.cuda()\n",
        "\n",
        "  summary(model, (opt[\"inputDim\"], opt[\"cropSize\"], opt[\"cropSize\"]))\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=opt[\"lr\"])\n",
        "\n",
        "  trainLoss, testLoss = AverageMeter(), AverageMeter()\n",
        "  trainAcc, testAcc = AverageMeter(), AverageMeter()\n",
        "  trainLossList, testLossList = [], []\n",
        "  trainAccList, testAccList = [], []\n",
        "  bestAcc = 0\n",
        "\n",
        "  for epoch in range(1, opt[\"epochs\"]+1) :\n",
        "    trainBar = tqdm(trainDataLoader)\n",
        "    trainLoss.reset(), trainAcc.reset()\n",
        "\n",
        "    for data in trainBar :\n",
        "      input, target = data[\"image\"], data[\"label\"]\n",
        "      if opt[\"isCUDA\"] :\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      pred = model(input)\n",
        "      loss = criterion(pred, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      trainLoss.update(loss.item(), opt[\"batchSize\"])\n",
        "      trainAcc.update(computeAcc(pred, target).item(), opt[\"batchSize\"])\n",
        "      trainBar.set_description(desc=f\"[{epoch}/{opt['epochs']}] [Train] < Accuracy:{trainAcc.avg:.6f} | Loss:{trainLoss.avg:.6f} >\")\n",
        "\n",
        "    trainLossList.append(trainLoss.avg)\n",
        "    trainAccList.append(trainAcc.avg)\n",
        "\n",
        "    testBar = tqdm(testDataLoader)\n",
        "    testLoss.reset(), testAcc.reset()\n",
        "\n",
        "    for data in testBar :\n",
        "      input, target = data[\"image\"], data[\"label\"]\n",
        "      if opt[\"isCUDA\"] :\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad() :\n",
        "        pred = model(input)\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        testLoss.update(loss.item(), opt[\"batchSize\"])\n",
        "        testAcc.update(computeAcc(pred, target).item(), opt[\"batchSize\"])\n",
        "        testBar.set_description(desc=f\"[{epoch}/{opt['epochs']}] [Test] < Accuracy:{testAcc.avg:.6f} | Loss:{testLoss.avg:.6f} >\")\n",
        "\n",
        "    testLossList.append(testLoss.avg)\n",
        "    testAccList.append(testAcc.avg)\n",
        "\n",
        "    if testAcc.avg > bestAcc :\n",
        "      bestAcc = testAcc.avg\n",
        "      torch.save(model.state_dict(), \"bestModel.pth\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"latestModel.pth\")\n",
        "\n",
        "  return (trainLossList, testLossList), (trainAccList, testAccList)"
      ],
      "metadata": {
        "id": "Toe0P0WH6c9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Training Option (Hyperparameter) Dictionary"
      ],
      "metadata": {
        "id": "ttFH7GYJq6c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\"dataRoot\":\"/content/\",\n",
        "       \"cropSize\":224,\n",
        "       \"seed\":42,\n",
        "       \"inputDim\":3,\n",
        "       \"targetDim\":4,\n",
        "       \"channels\":64,\n",
        "       \"batchSize\":16,\n",
        "       \"lr\":1e-4,\n",
        "       \"epochs\":10,\n",
        "       \"isCUDA\":torch.cuda.is_available()}"
      ],
      "metadata": {
        "id": "tXzQHG9aq9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "VpE3M2q67qRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lossList, accList = train(opt, myDataLoader, myModel, nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "id": "yV7Jgs_77c57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrgUhJC67uzu"
      },
      "source": [
        "## Plot Training vs. Validation Loss Graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_lIUGO11pAtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLsjFs3K7uzv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), lossList[0], label=\"Training Loss\")\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), lossList[1], label=\"Test Loss\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CE Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfMgjzvi9dYX"
      },
      "source": [
        "## Plot Training vs. Validation Accuracy Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0N1aiik9dYd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), accList[0], label=\"Training Accuracy\")\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), accList[1], label=\"Validation Accuracy\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S2Ag-GAuL2P"
      },
      "source": [
        "## Image Classification Model with Dropout and BN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHgSlZKZuL2P"
      },
      "outputs": [],
      "source": [
        "class myModel(nn.Module) :\n",
        "  def __init__(self, opt) :\n",
        "    super(myModel, self).__init__()\n",
        "\n",
        "    inputDim, targetDim, channels = opt[\"inputDim\"], opt[\"targetDim\"], opt[\"channels\"]\n",
        "\n",
        "    self.layer0 = nn.Sequential(nn.Conv2d(inputDim, channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(channels, channels*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels*2),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(channels*2, channels*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels*4),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels*4),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer4 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels*4),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer5 = nn.Sequential(nn.Conv2d(channels*4, channels*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                nn.BatchNorm2d(channels*4),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer6 = nn.Sequential(nn.Dropout(),\n",
        "                                nn.Linear(channels*4, targetDim))\n",
        "\n",
        "  def forward(self, input) :\n",
        "    output = self.layer0(input)\n",
        "    output = self.layer1(output)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    output = self.layer5(output)\n",
        "    output = F.adaptive_avg_pool2d(output, (1,1)).view(output.size(0), -1)\n",
        "    output = self.layer6(output)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regLossList, regAccList = train(opt, myDataLoader, myModel, nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "id": "M9WmT2-1uwp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Network vs. Network with Dropout & BN"
      ],
      "metadata": {
        "id": "MeELzQAQvwS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Graph"
      ],
      "metadata": {
        "id": "rN6yYbf6BnB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4e7bh6SvvUh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), lossList[1], label=\"Vanilla Model Loss\")\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), regLossList[1], label=\"Model with Dropout & BN Loss\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CE Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibN9ugUJvvUi"
      },
      "source": [
        "### Accuracy Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbq-zemKvvUi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), accList[1], label=\"Vanilla Model Loss\")\n",
        "plt.plot(np.arange(0, opt[\"epochs\"], 1), regAccList[1], label=\"Model with Dropout & BN Loss\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtpLMkdhS9+UhMOlRhneB/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}